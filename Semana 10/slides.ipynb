{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<p style=\"text-align:center;font-size:50px\"><b>Bertologia</b></p>\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/alan-barzilay/NLPortugues/blob/master/imagens/logo_nlportugues.png?raw=true\"  style=\"height:45%\" align=\"left\">\n",
    "\n",
    "<img src=\"http://ccsl.ime.usp.br/files/unmanaged/logos/logoIME+Texto.png\"  style=\"height:45%;width:10%\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "   - Momento Computer Vision em NLP: ULMFit\n",
    "   - Open AI Transformer\n",
    "   - BERT\n",
    "   - GPT-1 vs GPT-2 vs GPT-3 \n",
    "   - Bertologia\n",
    "   \n",
    "   (a ordem deveria ser essa?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRrZCqoP80uP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Momento Computer Vision em NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Um dos grandes motivos responsáveis para o _boom_ de Deep Learning em meados de 2014 foi uma técnica conhecida como **Transfer Learning**. Ela pode ser definida como a seguinte:\n",
    "\n",
    "> Transfer learning and domain adaptation refer to the situation where what has been learned in one setting … is exploited to improve generalization in another setting\n",
    "\n",
    "[Deep Learning Book](https://www.deeplearningbook.org/contents/representation.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/transfer_learning.jpeg'></center>\n",
    "\n",
    "Então, basicamente, a gente treina uma rede neural pesada para uma task grande, como o ImageNet, **pegamos os pesos da rede treinada** e treinamos uma nova rede para uma task menor, inicializando ela com esses pesos aprendidos na tarefa anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mas como isso é possível em NLP?\n",
    "\n",
    "A ideia de combinar a ideia de combinar transfer learning com o que ficou conhecido como aprendizado auto-surpervisionado.\n",
    "\n",
    "> Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text.\n",
    "\n",
    "[Fast.ai book- Chapter 10](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb)\n",
    "\n",
    "A tarefa, no caso de NLP, um tipo de modelo que entra nessa categoria de aprendizado auto surpervisionado é conhecida como **modelo de linguagem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelo de Linguagem\n",
    "\n",
    "Um modelo de linguagem é capaz de determinar a _fluência_ de um texto. Na prática, o desafio é, **com base nas palavras que eu vi até agora, qual deve ser a próxima palavra?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "P(w_n|w_0, w_1, w_2, \\dots, w_{n-1})\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Qual a intuição para isso funcionar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Regra da Cadeia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "P(w_0, w_1, w_2, \\dots, w_n) = P(w_0) \\times P(w_1|w_0) \\times P(w_2|w_0, w_1)  \\dots \\times P(w_n|w_0, w_1, w_2, \\dots, w_{n-1})\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "P(w_0, w_1, w_2, \\dots, w_n) = \\displaystyle \\prod_{i} P(w_n|w_0, w_1, w_2, \\dots, w_{n-1})\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "P(\"Eu gosto de abacate\") = P(\"Eu\") $\\times$ P(\"gosto|Eu\") $\\times$ P(\"de|Eu, gosto\") $\\times$ P(\"abacate|Eu, gosto, de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Percebam que tendo um modelo assim, a gente **não precisa de labels** e, logo, o próprio dado de treinamento pode ser usado para gerar as labels e, assim, treinar algo! Daí a ideia de **auto surpervisionado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='images/yann_lecunn_quote.jpg'  style=\"height:70%;width:40%\" align=\"left\">\n",
    "\n",
    "<img src='images/yann_lecunn_slides.jpg' align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ULMFit\n",
    "\n",
    "Em Janeiro de 2018, Sebastian Ruder e Jeremy Howard tiveram a ideia de combinar essas duas técnicas, modelo que eles nomearam de [**ULMFit**](https://arxiv.org/abs/1801.06146) ou _Universal Language Model Fine-tuning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/ulmfit.png)\n",
    "[fastai book](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A ideia é criar um modelo de Linguagem \"genérico\", treinado com base no Wikipedia. Disso, fazemos uma etapa de fine-tuning no nosso dataset objetivo, **com a ideia de que o modelo aprenda o linguajar específico desse domínio** e, então, fazemos uma etapa final de treinamento _focando na nossa tarefa final_, como um classificador de sentimentos de filmes por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mas e a Arquitetura?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para treinar esses modelos, foi utilizada a seguinte arquitetura:\n",
    "\n",
    "- [AWD LSTM](https://arxiv.org/pdf/1708.02182.pdf), que pode ser entendida como uma LSTM com algumas tecnicas a mais de otimização e regularização :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/ulmfit_example.png'  style=\"height:70%;width:70%\"></center>\n",
    "\n",
    "[overview ulmfit](https://humboldt-wi.github.io/blog/research/information_systems_1819/group4_ulmfit/#overviewulmfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por fim, a loss, utilizada é a boa e velha _Cross Entropy Loss_ :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# define model\n",
    "vocab_size = 1000\n",
    "embed_dim = 50  # tamanho do Embedding de cada token\n",
    "maxlen = 200  # Tamanho máximo da sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "tokenizer_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace', ngrams=None, output_mode=\"int\",\n",
    "    output_sequence_length=maxlen, pad_to_max_tokens=False)\n",
    "tokenizer_layer.adapt(np.array([\"O rato roeu a roupa do rei de roma\"]))\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name='input_text')\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = tokenizer_layer(inputs)    \n",
    "x = embedding_layer(x)\n",
    "x = layers.LSTM(units=1)(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwEekJH_80uV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mas atenção não é tudo que a gente precisa?\n",
    "\n",
    "Contudo, lembrem que na útlima aula vimos um paper que saiu em no meio de 2017, o **Attention is All you need**? Nele, foi apresentado a arquitetura Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='images/transformer.png'  style=\"height:20%;width:20%\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Acontece que a ideia tida por [Radford et. al](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) em Junho de 2018 foi a de usar a parte **decoder** da rede para o processo de aprendizado auto surpervisionado! Esse modelo ficou conhecido como **Open Ai Transformer**, ou GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='images/transformer-decoder.png'  style=\"height:20%;width:20%\"></center>\n",
    "\n",
    "[attention? attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#decoder)\n",
    "\n",
    "* A diferença é que no caso desse modelo do Open AI $N=12$ :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"images/TransformerDecoderExample.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"images/TransformerDecoderExample.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Basicamente, então, também é valido dizer que o GPT é um modelo **auto regressivo**. Ou sejam a medida que ele aprende uma palavra, **ele usa ela como informação para a próxima**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif'></center>\n",
    "\n",
    "[Illustrated GPT-2](http://jalammar.github.io/)\n",
    "\n",
    "As diferenças entre os GPT's são arquiteturais, como veremos daqui a pouco :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fine Tuning\n",
    "\n",
    "Seguindo, a ideia de pós treinamento auto surpervisionado, os autores propuseram fazer fine-tunings específicos, dependendo do tipo de task utilizada\n",
    "\n",
    "**PERGUNTA para o Finger: qual é a entrada da rede finetuning? Eu entendi que era o token [CLS] no caso do BERT, mas aqui isso nao ficou claro pra mim se é um token só ou as somas dos embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='images/open-ai-finetuing.png'></center>\n",
    "\n",
    "[open AI transformer paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferença aqui, é que no processo de Finetuing, dois novos Embeddings são adicionados: o de _start_ **\\<s\\>** e _extract_ **\\<e\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## E aí a grande revolução em NLP começou :)\n",
    "\n",
    "<center><img src='images/open-ai-finetuning-results.png' style=\"height:50%;width:50%\"></center>\n",
    "    \n",
    "[open AI transformer paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar do artigo do GPT-1 ser impressionante para a época, alguns meses depois, em Outubro do mesmo ano (o GPT foi lançado em Junho), o pessoal do Google propôs usar a parte **encoder** do transformer, junto de outras modificações leves, dando ao modelo o nome de **B**idirectional **E**ncoder **R**epresentations from **T**ransformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69ozd-JZ80uW",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Encoder Layer\n",
    "\n",
    "Relembrando o Encoder Layer dos Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='images/transformer-encoder.png'  style=\"height:30%;width:30%\"></center>\n",
    "\n",
    "[attention? attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#encoder)\n",
    "\n",
    "* A diferença é que no caso desse modelo do BERT $N=12$ ou $N=24$ :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGcHfkp380ua",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A primeira diferença é **abandonar o Mask Multi Head** e usar direto o **Multi Head Attention**, daí o nome birecional!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mas se fazemos isso, não estamos **overfittando** o modelo já que vamos olhar o contexto futuro para prever a palavra atual?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solução: usar **outra loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falta:\n",
    "- Loss: Masked Language Model Loss e NSP loss\n",
    "- GPT-2 vs GPT-3 (fala do Sparse Transformer(?))\n",
    "- Roberta; Albert; ELECTRA; T5; BART, passando por cima de todos\n",
    "- HuggingFace code examples\n",
    "- Tricks for training in GPUs: fp16; grandient accumulation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cópia de text_classification_with_transformer",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlportugues",
   "language": "python",
   "name": "nlportugues"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
